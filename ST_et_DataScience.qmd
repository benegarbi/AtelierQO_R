---
title: "Statistique textuelle et data Science"
format: 
  revealjs:
    theme: style.scss 
    scrollable: true
    smaller: true
---

## Programme

-   Statistique textuelle et dataScience
-   Corpus et Tableaux lexicaux
-   Méthodologie embarquée (dans R)
-   Application avec des réponses à une question ouverte issue de l'enquête "[Population, Espace de Vie, Environnement](https://data.ined.fr/index.php/catalog/41)" (Collomb, Guerin-Pace, Ined, 1992)
-   Illustrations à partir Garnier B., Guérin-Pace F. 2010 - Appliquer les méthodes de la statistique textuelle, \[[Ceped, les clefs pour](http://www.ceped.org/fr/publications-ressources/editions-du-ceped-1988-2012/les-clefs-pour/article/appliquer-les-methodes-de-la)\], Paris

------------------------------------------------------------------------

## Enjeux de la statistique (textuelle)

-   **Explorer** : *faire naître des idées*, détecter des similitudes, des différences, des anomalies, ....
-   **Résumer** les données à l'aide d' indicateurs, de profils
-   **Présenter** des résultats ...

<img src="images/im_nuage_couverture.png" height="200px"/>

mais aussi :

-   **Structurer** : le corpus en base de données et le **nettoyer**

------------------------------------------------------------------------

## Place de la Statistique textuelle

et si on allait voir ce que dit [Wikipedia](https://fr.wikipedia.org/wiki/Science_des_donn%C3%A9es) sur la Science de la donnée

![](images/wiki_DS.png){fig-align="center" width="642"}

------------------------------------------------------------------------

## Afficher des concordances

Le concordancier : indispensable tout au long d'une analyse de texte, quel qu'il soit :

<img src="images/im_concordancier_lexico3.png" height="500px"/>

------------------------------------------------------------------------

## Analyse quantitative de données qualitatives

Calculs d' **occurrences** = s'intéresser à la *forme* des textes en faisant abstraction de leur contenu

Ex : Attributions d'écrits historiques ou littéraires à un auteur, comparaison et évolution du style de différents auteurs, etc.

Recherche de **cooccurrences** = faire *émerger des structures* de textes au-delà de leur forme

Ex : Analyse des réponses à une question ouverte, analyse d'entretiens, de discours, etc.

S'appuyer sur des *métadonnées* sur les textes

------------------------------------------------------------------------

## L'Analyse des Données

<img src="images/im_benzecri.png" height="500px"/>

------------------------------------------------------------------------

## Statistique textuelle

Pour faire émerger des thématiques au moyen de méthodes statistiques d'analyses multivariées (AFC, CDH) **sans a priori**

Logiciels  *historiques*  (Spad, Lexico, Alceste, Hyperbase) aujourd'hui **open source** écrits *à partir de R* (tm, R.temis, TXM, Quanteda, IRaMuteQ ou Xplortext ....)

Les méthodes s'appliquent à des corpus qui diffèrent par leur nature mais qui sont transformés en tableaux de même structure : les **tableaux lexicaux**

------------------------------------------------------------------------

## Usage croissant

<img src="images/im_chronologie.png" width="683" height="500"/>

------------------------------------------------------------------------

## Text Mining

<img src="images/im_ricco.png" height="500px"/>

------------------------------------------------------------------------

## Topic Model

Modèle probabiliste permettant de déterminer des champs lexicaux dans un document (apprentissage automatique - traitement automatique du langage naturel (TLN))

<img src="images/im_topic_modeling.png" height="400px"/>

------------------------------------------------------------------------

## Chaîne de traitement de textes

<img src="images/im_flochart_text_analysis.png" height="400px"/>

<http://www.tidytextmining.com/topicmodeling.html>

------------------------------------------------------------------------

## Collecter Corpus et métadonnées

*quels sont les textes les plus semblables en ce qui concerne le vocabulaire et la fréquence des formes utilisées ? Quelles sont les formes qui caractérisent chaque texte, par leur présence ou leur absence ?* (Lebart & Salem, 1994, p.135)

Les questionner, les contextualiser - disponibilités/droits, sources, limites...

------------------------------------------------------------------------

## Nettoyer les données

= Etape de l'analyse *à ne pas sous-estimer*

Diffère selon les types de corpus (questions ouvertes, entretiens, romans, articles, pages Web etc..)

Textes et metadonnées = nettoyer, normaliser, corriger ( encodage, orthographe, abreviations ...)

------------------------------------------------------------------------

## Exemple de question ouverte dans un questionnaire

<img src="images/im_pee.png" height="500px"/>

------------------------------------------------------------------------

# Calcul d'occurrences

------------------------------------------------------------------------

## Le tableau lexical *entier* (TLE)

![](images/TLE_PEE_o.png){fig-align="center" width="538"}

Tableaux dits *hyper-creux*. Présence/absence de **mots** dans les textes (Valeur positive ou nulle). L'ordre des mots n'est pas pris en compte (sacs de mots)

------------------------------------------------------------------------

## Lecture du lexique

-   Les *mots* vont constituer le dictionnaire ou **lexique** associé au corpus et deviennent des descripteurs : les **termes**

![](images/liste_voc_pee.png){fig-align="center" height="400"}

Lecture des mots par ordre de fréquence*occurrence*, ordre *alphabétique*.

------------------------------------------------------------------------

## Méthodologie embarquée

Réduire la taille du lexique Via l'opération de **lemmatisation**

= rattacher un ou plusieurs mots à une forme dite racine (Lebart, Salem, 1994)

Convertir :

-   les formes verbales à l'infinitif
-   les substantifs au singulier
-   les adjectifs au masculin singulier

Opération **automatisée** avec des dictionnaires et/ou manuelle

------------------------------------------------------------------------

# Détection des cooccurrences

------------------------------------------------------------------------

## Analyse des correspondances sur le tableau lexical entier

Les plans factoriels permettent de visualiser des proximités de mots, des oppositions et ainsi de repérer des **champs lexicaux**.

![](images/spgeo_0046-2497_1998_num_27_1_T1_0044_0000_1.png){fig-align="center"}

(Enquête Population- Espaces de vie- Environnement, Ined, 1992) Deux mots sont d'autant plus proches que leurs contextes d'utilisation se ressemblent et d'autant plus éloignés qu'ils seront rarement utilisés ensemble

------------------------------------------------------------------------

## Classification sur Tableau Lexical

*Obtenir un classement des unités de textes en fonction de la ressemblance ou de la dissemblance des mots dans ces textes et d'ordonner les textes en cernant les homologies et les oppositions* (Rouré, Reinert, 1993)

![](images/spgeo_0046-2497_1998_num_27_1_T1_0046_0000_1.png){fig-align="center"}

Méthode Alceste ( Reinert, 1983), aujourd'hui implantée dans le *package Rainette* (J. Barnier)

------------------------------------------------------------------------
                                         
# Mettre en relation mots et métadonnées

------------------------------------------------------------------------

## Les spécificités

Utilisation d'un test statistique pour dire si l'écart entre la fréquence relative d'une forme dans une partition (*par modalité*) et la fréquence globale calculée sur l'ensemble des réponses est significatif ou non

![](images/specif.png){fig-align="center" width="429"}

les *mots ou textes caractéristiques* de ces partitions sont restitués selon leur degré de spécificité

------------------------------------------------------------------------

## Le tableau lexical agrégé (TLA)

Tableau de *contingence* qui croise les *mots* du lexique et les *modalités* des métadonnées.

![](images/TLA_PEE.png){fig-align="center" width="960"}

(Population, Espace de vie, Environnement,Ined)

------------------------------------------------------------------------

## Analyse des correspondances sur un Tableau Lexical Agrégé

Le plan factoriel permet d'observer la position réciproque des "mots" et des métadonnées et de faire émerger des champs lexicaux propres à des sous-populations

<img src="images/spgeo_0046-2497_1998_num_27_1_T1_0050_0000_1.png" height="500px"/>

-   2 mots proches = proximité des individus - profils lignes
-   2 caractéristiques proches = univers lexicaux proches - profils colonnes

------------------------------------------------------------------------

## Affiner l'analyse

- Supprimer certains mots ....
- Augmenter le nombre de classes
- Personnaliser la lemmatisation .....
- Extraire des sous-corpus à l'aide metadonnées

------------------------------------------------------------------------

## Les outils

Liste non exhaustive

![](images/im_outils.png){fig-align="center" height="400"}

------------------------------------------------------------------------

## Package tm (Text Mining) de R

Feinerer, Hornik, Meyer Wirtschaftsuniversity de Wien, in Journal of Statistical Software (Mars 2008)

-   Construction de tableaux lexicaux (**Document Term Matrix**), comptage de mots, calcul d'associations, ... = fonctions de tm
-   Rapporte les mots à leurs radicaux (stemming) ou supprime les mots outils (i.e articles) = options de tm

------------------------------------------------------------------------

## Package R.temis de R

Facilite les étapes essentielles de l'analyse textuelle en s'appuyant au maximum sur les packages existants (tm, FactoMineR, explor, igraph...). [R.Temis](https://rtemis.hypotheses.org/) implémente les méthodes suivantes :

-   importation de corpus au format .csv, .txt, Alceste
-   suppression des mots vides,
-   lemmatisation automatique et modifiable
-   bilan lexical, spécificités, concordances
-   nuage de mots
-   détection de cooccurrences,
-   construction de sous-corpus à partir de termes
-   découpage des textes en paragraphes
-   analyse des correspondances sur tableau lexical
-   classification
-   graphes de mots

------------------------------------------------------------------------

## Conclusion

-   Analyse de données (non structurées)
-   Explorer les données autrement - sans a priori
-   Complémentarité des méthodes (qualitative/quantitative)
-   Utilisation conjointe de l'informatique tout-automatique et de l'intuition humaine

------------------------------------------------------------------------

## Statistique textuelle : Quali + Quanti + Viz

Calculs statistiques appliqués à des **corpus**

-   Chiffres & Mots : **Occurrences & Cooccurrences**, ...

-   Calcul de **spécificités**, profils, ...

-   **Visualisations** : nuages de mots, graphe de mots, plan factoriels (Analyse des correspondances), dendrogrammes (classifications)

Aides à l'interprétation indispensables : les **concordances**
